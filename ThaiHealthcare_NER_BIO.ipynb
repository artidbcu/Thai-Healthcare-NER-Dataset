{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# for Google Colab use A100 GPU"
      ],
      "metadata": {
        "id": "9h_uBReMHM8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 1:** Setup Environment and import neccesory libraries"
      ],
      "metadata": {
        "id": "rDRC-2-wyLJN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "3DhVRvMIrkEf"
      },
      "outputs": [],
      "source": [
        "!pip install tltk\n",
        "!pip install pythainlp\n",
        "!pip install llama-index-llms-openai-like\n",
        "!pip install --upgrade openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FpD8wPmKr_Ts"
      },
      "outputs": [],
      "source": [
        "import json, re, time, math\n",
        "from pythainlp.tokenize import word_tokenize\n",
        "from pythainlp.tag import pos_tag\n",
        "from tqdm import tqdm\n",
        "from llama_index.core.llms import ChatMessage, MessageRole\n",
        "from llama_index.llms.openai_like import OpenAILike\n",
        "from openai import OpenAI, APIError\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7PYZXmAssDb"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "  text = re.sub(r'<.*?>','', text)\n",
        "  text = re.sub(r'#','',text)\n",
        "  #for c in string.punctuation:\n",
        "      #text = re.sub(r'\\{}'.format(c),'',text)\n",
        "  text = text.replace(\"[\", '')\n",
        "  text = text.replace(\"]\", '')\n",
        "  text = text.replace(\"(\", '')\n",
        "  text = text.replace(\")\", '')\n",
        "  text = text.replace(\"=\", '')\n",
        "  text = text.replace('“', '')\n",
        "  text = text.replace('”', '')\n",
        "\n",
        "  text = ' '.join(text.split()) # remove separator ex. \\n \\t\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LeSibSnAtNg2"
      },
      "outputs": [],
      "source": [
        "def compound_word_constructor(word_list):\n",
        "  noun_list = [\"NCMN\", \"NPRP\"]\n",
        "  verb_list = [\"VACT\", \"VSTA\"]\n",
        "  prefix_list = [\"FIXN\"]\n",
        "  preposition_list = [\"RPRE\"]\n",
        "  adverb_list = [\"ADVN\"]\n",
        "  end_list = [\"PUNC\"]\n",
        "  verb_to_be = [\"เป็น\", \"อยู่\", \"คือ\"]\n",
        "\n",
        "  i = 0\n",
        "  prev_tag = \"\"\n",
        "  next_tag = \"\"\n",
        "  compound_word = []\n",
        "  compound_word_list = []\n",
        "  tokens_list = []\n",
        "  for word, tag in word_list:\n",
        "    if(len(word_list) == 2):\n",
        "      if(tag in noun_list):\n",
        "        compound_word = []\n",
        "      tokens_list.append(word)\n",
        "    else:\n",
        "      if(len(compound_word) == 0):\n",
        "        if(tag in noun_list or tag in prefix_list):\n",
        "          compound_word.append(word)\n",
        "        else:\n",
        "          tokens_list.append(word)\n",
        "      else:\n",
        "        if(tag in end_list):\n",
        "          compound_word_list.append(\"\".join(compound_word))\n",
        "          tokens_list.append(\"\".join(compound_word))\n",
        "          compound_word = []\n",
        "          tokens_list.append(word)\n",
        "        else:\n",
        "          if(i < len(word_list) - 1):\n",
        "            next_tag = word_list[i+1][1]\n",
        "            prev_tag = word_list[i-1][1]\n",
        "            if(tag in noun_list):\n",
        "              compound_word.append(word)\n",
        "            elif(tag in verb_list):\n",
        "              if((prev_tag in prefix_list) and (tag not in verb_to_be)):\n",
        "                compound_word.append(word)\n",
        "              elif((prev_tag in noun_list) and (next_tag in noun_list or next_tag in end_list) and (tag not in verb_to_be)):\n",
        "                compound_word.append(word)\n",
        "              else:\n",
        "                compound_word_list.append(\"\".join(compound_word))\n",
        "                tokens_list.append(\"\".join(compound_word))\n",
        "                compound_word = []\n",
        "            elif(tag in prefix_list):\n",
        "              if(next_tag in verb_list): compound_word.append(word)\n",
        "              else:\n",
        "                compound_word_list.append(\"\".join(compound_word))\n",
        "                tokens_list.append(\"\".join(compound_word))\n",
        "                compound_word = []\n",
        "                compound_word.append(word)\n",
        "            elif(tag in adverb_list):\n",
        "              if(prev_tag in noun_list): compound_word.append(word)\n",
        "            elif(tag in preposition_list):\n",
        "              if((prev_tag in noun_list or prev_tag in verb_list) and (next_tag in noun_list or next_tag in prefix_list)):\n",
        "                compound_word.append(word)\n",
        "              else:\n",
        "                compound_word_list.append(\"\".join(compound_word))\n",
        "                tokens_list.append(\"\".join(compound_word))\n",
        "                compound_word = []\n",
        "            else:\n",
        "              compound_word_list.append(\"\".join(compound_word))\n",
        "              tokens_list.append(\"\".join(compound_word))\n",
        "              compound_word = []\n",
        "          else:\n",
        "            prev_tag = word_list[i-1][1]\n",
        "            if(tag in noun_list):\n",
        "              compound_word.append(word)\n",
        "            elif((tag in verb_list) and (prev_tag in prefix_list)):\n",
        "              compound_word.append(word)\n",
        "            elif((tag in adverb_list) and (prev_tag in noun_list)):\n",
        "              compound_word.append(word)\n",
        "            compound_word_list.append(\"\".join(compound_word))\n",
        "            tokens_list.append(\"\".join(compound_word))\n",
        "            compound_word = []\n",
        "\n",
        "    i += 1\n",
        "\n",
        "  return [compound_word_list, tokens_list]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 2:** Load sample data"
      ],
      "metadata": {
        "id": "6E4pd-9Gyux9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZQbZPoIsMqg"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5aF9sJPwsqMj"
      },
      "outputs": [],
      "source": [
        "data_samples = []\n",
        "file = open('/content/drive/MyDrive/Thai-Healthcare-Dataset/sample_articles.json', encoding=\"utf8\")\n",
        "data = json.load(file)\n",
        "samples = data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 3:** Generate the generative words and use Typhoon suggest the SNOMED CT"
      ],
      "metadata": {
        "id": "f1My8E5_y9n3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6EyqsXkGuQCG"
      },
      "outputs": [],
      "source": [
        "def generate_words(text):\n",
        "  words = word_tokenize(text, engine=\"tltk\")\n",
        "  tags = pos_tag(words)\n",
        "  generative_words, tokens = compound_word_constructor(tags)\n",
        "\n",
        "  return generative_words, tokens"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_role = \"You are Named Entity Recognition (NER) annotator named ThaiNer. You are an expert in the Thai healthcare domian. Youalways answer in Thai\""
      ],
      "metadata": {
        "id": "i0vV6A5qOs55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "openAI = OpenAI(api_key=\"YOUR_OPENAI_API\")"
      ],
      "metadata": {
        "id": "92c0UqbTP5s_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "typhoon = OpenAILike(model=\"typhoon-v2-70b-instruct\",\n",
        "                api_base=\"https://api.opentyphoon.ai/v1\",\n",
        "                context_window=8192,\n",
        "                is_chat_model=True,\n",
        "                max_tokens=2048,\n",
        "                is_function_calling_model=False,\n",
        "                request_timeout=180,\n",
        "                max_new_tokens=2046,\n",
        "                api_key=\"sk-h2NoABRy5xUDtmkBvq0UW875Z30ujPM1JKcka3JtReHtpNIT\")\n",
        "response = typhoon.chat([ChatMessage(role=MessageRole.SYSTEM, content=prompt_role)])"
      ],
      "metadata": {
        "id": "D92FAsZwP7kh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getPromptNER(words):\n",
        "  return f'''\n",
        "Please annotate each word in list below:\n",
        "{words}\n",
        "\n",
        "follow a list of the Top Level Concepts of SNOMED CT with a brief description of the content represented in their branch of the hierarchy below:\n",
        "1. Body structure represents normal and abnormal anatomical structures (e.g. mitral valve structure, adenosarcoma).\n",
        "2. Clinical finding represents the result of a clinical observation, assessment or judgment and includes normal and abnormal clinical states (e.g. asthma, headache, normal breath sounds). The clinical findin|hierarchy includes concepts used to represent diagnoses.\n",
        "3. Environments and geographical locations represents types of environments as well as named locations such as countries, states and regions (e.g. intensive care unit, academic medical center, Denmark).\n",
        "4. Event represents occurrences excluding procedures and interventions (e.g. flood, earthquake).\n",
        "5. Observable entity represents a question or assessment which can produce an answer or result (e.g. systolic blood pressure, color of iris, gender).\n",
        "6. Organism represents organisms of significance in human medicine (e.g. streptococcus pyogenes, domain Bacteria, glossina).\n",
        "7. Pharmaceutical/biologic product represents drug products (e.g. amoxicillin 250mg oral capsule, product containing codeine and paracetamol).\n",
        "8. Physical force represents physical forces that can play a role as mechanisms of injury (e.g. friction, radiation, alternating current).\n",
        "9. Physical object represents natural and man-made physical objects (e.g. vena cava filter, implant device, automobile).\n",
        "10. Procedure represents activities performed in the provision of health care. This includes not only invasive procedures but also administration of medicines, imaging, education, therapies and administrative procedures (e.g. appendectomy, physiotherapy, injection into subcutaneous tissue).\n",
        "11. Qualifier value represents the values for some SNOMED CT attributes, where those values are not subtypes of other top level concepts. (e.g. left, abnormal result, severe).\n",
        "12. Record artifact represents content created for the purpose of providing other people with information about record events or states of affairs. (e.g. patient held record, record entry, family history section).\n",
        "13. Situation with explicit context represents concepts in which the clinical context is specified as part of the definition of the concept itself. These include presence or absence of a condition, whether a clinical finding is current, in the past or relates to someone other than the subject of the record (e.g. endoscopy arranged, past history of myocardial infarction, family history of glaucoma).\n",
        "14. Social context represents social conditions and circumstances significant to health care (e.g. occupation, spiritual or religious belief).\n",
        "15. Special concept represents concepts that do not play a part in the formal logic of the concept model of the terminology, but which may be useful for specific use cases (e.g. navigational concept, alternative medicine poisoning).\n",
        "16. Specimen represents entities that are obtained (usually from the patient) for examination or analysis (e.g. urine specimen, specimen from prostate obtained by needle biopsy).\n",
        "17. Staging and scales represents assessment scales and tumor staging systems (e.g. Glasgow Coma Scale, FIGO staging system of gynecological malignancy).\n",
        "18. Substance represents general substances, the chemical constituents of pharmaceutical/biological products, body substances, dietary substances and diagnostic substances (e.g. methane, insulin, albumin)\n",
        "\n",
        "Remarks: If not matching any concept, please annotate the word as None. Do not annotate values other than the Top Level Concepts list.\n",
        "\n",
        "Strictly return it in the following JSON format:\n",
        "{{\"entities\":[{{\"e\":\"word\",\"t\":\"type\"}}]}}\n",
        "\n",
        "ตัวอย่าง:\n",
        "Input: \"ผู้ป่วยมีอาการไข้และไอ ใช้ยาพาราเซตามอล\"\n",
        "Output: {{\"entities\":[{{\"e\":\"ผู้ป่วย\",\"t\": \"None\"}},{{\"e\":\"อาการไข้\",\"t\":\"Clinical finding\"}},{{\"e\":\"ไอ\",\"t\":\"Clinical finding\"}},{{\"e\":\"ยาพาราเซตามอล\",\"t\":\"Pharmaceutical/biologic product\"}}]}}\n",
        "'''"
      ],
      "metadata": {
        "id": "3RGmH2_-QFtE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retryWithBackoff(func, retries=3, initial_delay=1, backoff_factor=2):\n",
        "  for i in range(retries):\n",
        "    try:\n",
        "      return func()\n",
        "    except APIError:  # Catch both APIError and APITimeoutError\n",
        "      if i < retries - 1:\n",
        "        time.sleep(initial_delay * (backoff_factor ** i))\n",
        "        print(f\"Retrying ({i + 1}/{retries}) after {initial_delay * (backoff_factor ** i)} seconds...\")\n",
        "      else:\n",
        "        raise APIError\n",
        "\n",
        "def annotateWithTyphoon(text):\n",
        "  prompt = getPromptNER(text)\n",
        "  try:\n",
        "    response = typhoon.chat([ChatMessage(role=MessageRole.USER, content=prompt)])\n",
        "    message = response.message.content\n",
        "    return message\n",
        "  except APIError:\n",
        "    print(\"Error\")\n",
        "    return \"\"\n",
        "\n",
        "def annotateLabel(words):\n",
        "  labels = []\n",
        "  slide = 10\n",
        "  loop = math.floor(len(words)/slide)\n",
        "  for i in range(loop-1):\n",
        "    if((i+1 * slide) <= len(words)):\n",
        "      slide = 10\n",
        "    else:\n",
        "      slide = len(words)%10\n",
        "    if(slide > 0):\n",
        "      words_list = []\n",
        "      for j in list(range(10)):\n",
        "        words_list.append(words.pop(j))\n",
        "      prompt_words = \",\".join(words_list)\n",
        "      extract_labels = retryWithBackoff(lambda: annotateWithTyphoon(prompt_words))\n",
        "      try:\n",
        "        for label in json.loads(extract_labels)['entities']:\n",
        "          labels.append(label)\n",
        "      except json.JSONDecodeError:\n",
        "        print('Error')\n",
        "\n",
        "  return labels"
      ],
      "metadata": {
        "id": "t7GGY0TNSByp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "snomed_datasets = []\n",
        "for n in tqdm(range(0, 500)):\n",
        "  labels = []\n",
        "  sample = samples[n]\n",
        "  if \"abstract-th\" in sample:\n",
        "    if(sample[\"abstract-th\"] != \"\"):\n",
        "      text = clean_text(sample[\"abstract-th\"])\n",
        "      generative_words, tokens = generate_words(text)\n",
        "      labels = annotateLabel(generative_words)\n",
        "\n",
        "      entity_dict = defaultdict(set)\n",
        "      for label in labels:\n",
        "        entity_dict[label[\"t\"]].add(label[\"e\"])\n",
        "\n",
        "      snomed_types = {label: list(types) for label, types in entity_dict.items()}\n",
        "\n",
        "      snomed_datasets.append({\"text\":text, \"types\": snomed_types})\n",
        "\n",
        "  n = n+1"
      ],
      "metadata": {
        "collapsed": true,
        "id": "0Wlv6nxZSptp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sortByStart(e):\n",
        "  return e['start']"
      ],
      "metadata": {
        "id": "JeBbTeXG90x8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 4:** Group the genertive keywords aligned with SNOMED CT"
      ],
      "metadata": {
        "id": "ECoMzZlOznev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "snomed_type = [\"Body structure\", \"Clinical finding\", \"Environments and geographical locations\", \"Event\", \"Observable entity\",\n",
        "               \"Organism\", \"Pharmaceutical/biologic product\", \"Physical force\", \"Physical object\", \"Procedure\",\n",
        "               \"Qualifier value\", \"Record artifact\", \"Situation with explicit context\",  \"Social context\",  \"Special concept\",\n",
        "               \"Specimen\", \"Staging and scales\", \"Substance\"]"
      ],
      "metadata": {
        "id": "MSZLxX8O93L3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "snomed_type_count = {\"Body structure\": 0, \"Clinical finding\": 0, \"Environments and geographical locations\": 0, \"Event\": 0, \"Observable entity\": 0,\n",
        "               \"Organism\": 0, \"Pharmaceutical/biologic product\": 0, \"Physical force\": 0, \"Physical object\": 0, \"Procedure\": 0,\n",
        "               \"Qualifier value\": 0, \"Record artifact\": 0, \"Situation with explicit context\": 0,  \"Social context\": 0,  \"Special concept\": 0,\n",
        "               \"Specimen\": 0, \"Staging and scales\": 0, \"Substance\": 0}"
      ],
      "metadata": {
        "id": "J0DYSgo7_ZPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = 0\n",
        "position_datasets = {}\n",
        "for n in tqdm(range(len(snomed_datasets))):\n",
        "  text = clean_text(snomed_datasets[n]['text'])\n",
        "  types = snomed_datasets[n]['types']\n",
        "  entities = []\n",
        "  tokens = []\n",
        "  position_datasets[n] = {\"text\": text, \"entities\": [], \"tokens\": []}\n",
        "\n",
        "  for key in types:\n",
        "    if key in snomed_type:\n",
        "      for item in types[key]:\n",
        "        snomed_type_count[key] = snomed_type_count[key] + 1\n",
        "        char_idx = 0\n",
        "        token_end = 0\n",
        "        token_num = text.count(item)\n",
        "        token_list = []\n",
        "        for k in range(token_num):\n",
        "          token_start = text.find(item, char_idx)\n",
        "          token_end = token_start + len(item)\n",
        "          char_idx = token_end\n",
        "          token_list.append([item, token_start, token_end])\n",
        "\n",
        "        i = 0\n",
        "        for token in token_list:\n",
        "          token_start = token[1]\n",
        "          token_end = token[2]\n",
        "          token_text = token[0]\n",
        "\n",
        "          dup = False\n",
        "          if len(entities) > 0:\n",
        "            for j, ent in enumerate(entities):\n",
        "              if token_start == ent[\"start\"]:\n",
        "                if token_end < ent[\"end\"]:\n",
        "                  dup = True\n",
        "                elif token_end > ent[\"end\"]:\n",
        "                  entities[j][\"start\"] = token_start\n",
        "                  entities[j][\"end\"] = token_end\n",
        "                  entities[j][\"type\"] = key\n",
        "                  entities[j][\"token\"] = token_text\n",
        "                  dup = True\n",
        "                else:\n",
        "                  dup = True\n",
        "\n",
        "              if token_end == ent[\"end\"]:\n",
        "                if token_start > ent[\"start\"]:\n",
        "                  dup = True\n",
        "                elif token_start < ent[\"start\"]:\n",
        "                  entities[j][\"start\"] = token_start\n",
        "                  entities[j][\"end\"] = token_end\n",
        "                  entities[j][\"type\"] = key\n",
        "                  entities[j][\"token\"] = token_text\n",
        "                  dup = True\n",
        "                else:\n",
        "                  dup = True\n",
        "\n",
        "              if token_start > ent[\"start\"] and token_end < ent[\"end\"]:\n",
        "                dup = True\n",
        "\n",
        "              if token_start < ent[\"start\"] and token_end > ent[\"end\"]:\n",
        "                dup = True\n",
        "\n",
        "              j = j+1\n",
        "\n",
        "          if dup == False:\n",
        "            entities.append({\"start\": token_start, \"end\": token_end, \"type\": key, \"token\": item})\n",
        "\n",
        "          i = i+1\n",
        "  entities.sort(key=sortByStart)\n",
        "\n",
        "  new_set = entities\n",
        "  unique_data = list({json.dumps(item, ensure_ascii=False): item for item in new_set}.values())\n",
        "\n",
        "  # If you want to sort them (optional)\n",
        "  unique_data = sorted(unique_data, key=lambda x: (x['start'], x['end']))\n",
        "\n",
        "  new_token_list = []\n",
        "  for item in unique_data:\n",
        "      new_token_list.append(item)\n",
        "      tokens.append(item['token'])\n",
        "  m = 0\n",
        "  newlen = len(new_token_list)\n",
        "  for item in new_token_list:\n",
        "    if m == newlen-1:\n",
        "      break\n",
        "    else:\n",
        "      next_token = new_token_list[m+1]\n",
        "      if next_token['start'] > item[\"start\"] and next_token[\"end\"] < item[\"end\"]:\n",
        "        del new_token_list[m+1]\n",
        "        newlen = len(new_token_list)\n",
        "      if (next_token['start'] > item[\"start\"] and next_token['start'] <= item[\"end\"]) and next_token[\"end\"] > item[\"end\"]:\n",
        "        del new_token_list[m+1]\n",
        "        newlen = len(new_token_list)\n",
        "    m = m+1\n",
        "\n",
        "  position_datasets[n][\"entities\"] = new_token_list\n",
        "  position_datasets[n][\"tokens\"] = list(set(tokens))\n",
        "\n",
        "  n = n+1"
      ],
      "metadata": {
        "id": "xm2Q79fr-dsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 5:** Create BIO-tagged dataset"
      ],
      "metadata": {
        "id": "V27pqvt2z74R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bio_tags = []\n",
        "item = 0\n",
        "for item in tqdm(position_datasets):\n",
        "  data = position_datasets[item]\n",
        "  text = clean_text(data[\"text\"])\n",
        "  text_len = len(text)\n",
        "  entityies_len = len(data[\"entities\"])\n",
        "  dataset = []\n",
        "  startidx = 0\n",
        "  endidx = 0\n",
        "  for i, entity in enumerate(data[\"entities\"]):\n",
        "\n",
        "    if(i == 0):\n",
        "      #first token\n",
        "      if(entity['start'] > 0):\n",
        "        startidx = 0\n",
        "        endidx = entity['start']\n",
        "        sentence = text[startidx:endidx]\n",
        "        tokens = word_tokenize(sentence, engine=\"tltk\", keep_whitespace=False)\n",
        "        for token in tokens:\n",
        "          dataset.append([token, 'O'])\n",
        "        dataset.append([entity['token'], entity['type']])\n",
        "        startidx = entity['end']\n",
        "      else:\n",
        "        startidx = entity['start']\n",
        "        endidx = entity['end']\n",
        "        dataset.append([entity['token'], entity['type']])\n",
        "        startidx = endidx\n",
        "    else:\n",
        "      if(entity['start'] == startidx):\n",
        "        #next token not O tag\n",
        "        endidx = entity['end']\n",
        "        dataset.append([entity['token'], entity['type']])\n",
        "        startidx = endidx\n",
        "      else:\n",
        "        #next token is O tag\n",
        "        if(i == entityies_len-1):\n",
        "          #last token\n",
        "          if(entity['end'] < text_len):\n",
        "            #lase token with O tag\n",
        "            dataset.append([entity['token'], entity['type']])\n",
        "            startidx = entity['end']\n",
        "            endidx = text_len\n",
        "            sentence = text[startidx:endidx]\n",
        "            tokens = word_tokenize(sentence, engine=\"tltk\", keep_whitespace=False)\n",
        "            for token in tokens:\n",
        "              dataset.append([token, 'O'])\n",
        "          else:\n",
        "            #last token without O tag\n",
        "            dataset.append([entity['token'], entity['type']])\n",
        "        else:\n",
        "          #not last token\n",
        "          next_entity = data[\"entities\"][i+1]\n",
        "          if(entity['start'] > startidx):\n",
        "            sentence = text[startidx:entity['start']]\n",
        "            tokens = word_tokenize(sentence, engine=\"tltk\", keep_whitespace=False)\n",
        "            for token in tokens:\n",
        "              dataset.append([token, 'O'])\n",
        "            dataset.append([entity['token'], entity['type']])\n",
        "            startidx = entity['end']\n",
        "          else:\n",
        "            startidx = entity['start']\n",
        "            endidx = entity['end']\n",
        "            dataset.append([entity['token'], entity['type']])\n",
        "            startidx = endidx\n",
        "  bio_tags.append(dataset)"
      ],
      "metadata": {
        "id": "hNJHHaKX-xeV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def align_tokens_to_bio(text, tokens, entities):\n",
        "  types = ['O'] * len(tokens)\n",
        "  char_idx = 0\n",
        "  new_tokens = []\n",
        "  entity_data = []\n",
        "\n",
        "  for i, token in enumerate(tokens):\n",
        "    if token.find(\"<Fail>\") < 0:\n",
        "      new_tokens.append(token)\n",
        "      token_start = text.find(token, char_idx)\n",
        "      token_end = token_start + len(token)\n",
        "      char_idx = token_end\n",
        "\n",
        "      for ent in entities:\n",
        "        if token_start >= ent[\"start\"] and token_end <= ent[\"end\"]:\n",
        "          if token_start == ent[\"start\"]:\n",
        "            types[i] = f\"B-{ent['type']}\"\n",
        "          else:\n",
        "            types[i] = f\"I-{ent['type']}\"\n",
        "          entity_data.append(\"type:\"+types[i]+\" token:\"+token)\n",
        "  return [new_tokens, types, entity_data]"
      ],
      "metadata": {
        "id": "UVoqKGoYBuRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bio_datasets = []\n",
        "for entry in tqdm(position_datasets.values()):\n",
        "  text = clean_text(entry[\"text\"])\n",
        "  entities = entry[\"entities\"]\n",
        "\n",
        "  tokens = word_tokenize(text, engine=\"tltk\", keep_whitespace=True)\n",
        "  ner_tags = align_tokens_to_bio(text, tokens, entities)\n",
        "\n",
        "  bio_datasets.append({\n",
        "      \"lang\": \"th\",\n",
        "      \"tokens\": ner_tags[0],\n",
        "      \"ner_tags\": ner_tags[1],\n",
        "      \"text\": text\n",
        "  })"
      ],
      "metadata": {
        "id": "rYuxYllTBzcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/Thai-Healthcare-Dataset/bio_dataset_500.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(bio_datasets, f, ensure_ascii=False, indent=2)"
      ],
      "metadata": {
        "id": "JLbKnvH7PItY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 6:** Create instaruction-tuned dataset"
      ],
      "metadata": {
        "id": "pffuzfyL0Ddx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bio_list_to_html_bio(tokens, labels):\n",
        "    result = []\n",
        "    for token, label in zip(tokens, labels):\n",
        "        label = label.strip()\n",
        "        token = token.strip()\n",
        "\n",
        "        if label == \"O\":\n",
        "            result.append(f\"<O>{token}</O>\")\n",
        "        else:\n",
        "            result.append(f\"<{label}>{token}</{label}>\")\n",
        "    return \"\".join(result)"
      ],
      "metadata": {
        "id": "nyu9BUdR0tN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_instruction_prompt(text):\n",
        "    return (\n",
        "        \"Annotate the given Thai medical text using the BIO tagging format according to SNOMED CT top-level concepts. \"\n",
        "        \"Wrap each word using tags like <O>...</O>, <B-Label>...</B-Label>, <I-Label>...</I-Label>.\\n\\n\"\n",
        "        \"Input:\\n\" + text + \"\\n\\nOutput:\"\n",
        "    )"
      ],
      "metadata": {
        "id": "4cSL8RzU0wRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "converted_data = []\n",
        "for item in bio_datasets:\n",
        "    input_text = item[\"text\"]\n",
        "    html_output = bio_list_to_html_bio(item[\"tokens\"], item[\"ner_tags\"])\n",
        "    converted_data.append({\n",
        "        \"instruction\": create_instruction_prompt(item),\n",
        "        \"input\": input_text,\n",
        "        \"output\": html_output\n",
        "    })"
      ],
      "metadata": {
        "id": "WHYpfryM0-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/Thai-Healthcare-Dataset/thai_healthcare_instruction.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(converted_data, f, ensure_ascii=False, indent=2)"
      ],
      "metadata": {
        "id": "cONOIRDN1-VF"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}